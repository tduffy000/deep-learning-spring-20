## Fundamentals of ML with NN
1. We want to build an autoencoder with a squeeze layer in the hidden dimension that will project our input features to a smaller latent space so we can use those embeddings as features themselves. Here, we are attempting to perform:
- **Dimension reduction**
- Supervised learning
- Reinforcement Learning
- Regularization

2. We call *L<sub>1</sub>* and *L<sub>2</sub>* regularization, respectively:
- Ridge and Lasso
- Norm and Binorm
- **Lasso and Ridge**
- Least Squares and LOESS

## Convnet Deep Learning Computer Vision

1. How can we use a pre-trained classifier (like VGG16) trained on an entirely different dataset relevantly in our custom classification problem?
- By copying the architecture are training it on our dataset just changing the input size
- By adding the dataset that model was trained on to ours and training it on their union.
- **By using the weights of the convolution layers as feature mappings and adding our own fully connected network on top.**
- We shouldn't it's not really relevant.

2. Which one of the following is *not* a viable data augmentation technique for images?
- Vertically flipping images.
- Rotating the images a random number of degrees.
- Fluctuate the RGB intensities of images.
- **Remove certain images from our training set.**

## RNN and working with Text

1. What architectural detail is different between n-gram and transformers which allowed
the latter to perform better at placing words into context?
- Transformers throw away generic words like "the" and "it".
- **Transformers don't form sequential word sequences, but take permutations of a sentence.**
- Transformers translate sentences into multiple languages to determine context.
- Transformers are more compact than certain n-grams.

2. What is the nice thing you get from embeddings generated by word2vec which is useful
for downstream tasks or as inputs to other classifiers?
- Certain words are joined together if they mean the same thing.
- **A notion of similarity / distance in the output space the words are projected to.**
- The can't be used as features.
- They are much smaller dimensionally.

## Advanced Deep Learning Practices

1. I want to perform some (or a group of) operation(s) at the end of each training epoch
with the `tf.keras` API. What part of the API allows me to register functions that will be
executed after each epoch?
- `tf.keras.optimizer`
- `tf.keras.loss`
- `tf.keras.callbacks` **<--**
- `tf.keras.functions`

2. Which dynamic present in neural network architecture was addressed by batch normalization?
- The exploding gradient problem.  
- The class stratification problem.
- **The shifting distribution of upstream layer's weights**
- The shifting distribuion of downstream layer's weights

## Generative Networks

1. GANs were motivated by which of the following concepts / algorithms from Game Theory
- **Minimax**
- Nash's algorithm
- Goodman's algorithm
- Expectimax

2. If we want to perform style transfer of an image of a dog into the style of a Van Gogh
painted, what we call the image of the dog (if the Van Gogh painting is the style reference)?
- The latent image.
- The reference image.
- The base image.
- **The content image.**

## Reinforcement Learning

1. A Markov chain / process has *this* particular property with respect that relates **P**(X<sub>t+1</sub> | X<sub>t</sub>) and **P**(X<sub>t+1</sub> | X<sub>t-1</sub>)
if our current state is *t*:
- **The state of the system at *t-1* is irrelevant at *t* w.r.t probability of *t+1***
- It is monotonically increasing as a function of $t$
- It says that we can no longer return to the state of the system at *t-1* from *t+1*
- It is saturating

2. In board games engines (e.g. chess or Go) we have to determine the value of a particular
board by treating it as an
- Optimization problem
- **Search problem**
- Intractable problem
- Gradient descent problem

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This will be revised with a jupyter notebook to work in but here is the problem so you can get started.\n",
    "Please note the following dataset repositories:\n",
    "\n",
    "    http://archive.ics.uci.edu/ml/\n",
    "    https://www.kaggle.com/datasets\n",
    "    https://registry.opendata.aws\n",
    "    http://dataportals.org/\n",
    "    http://opendatamonitor.eu/\n",
    "    http://quandl.com/\n",
    "    https://homl.info/9\n",
    "    https://homl.info/10\n",
    "    https://www.reddit.com/r/datasets\n",
    "\n",
    "These will generally be useful, possibly in your day job and absolutely for side projects. Coming across good data is hard to do! Fortunately others have taken care of this for us!\n",
    " \n",
    "The following dataset was taken from the first dataset repository: http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "As the original task of the dataset lays out, the goal is try and classify whether or not an individual is likely to make more or less than 50K per year. Carry out this task. Try at least five general architectures, report precision, recall and f1 score on a test set.\n",
    "Please note:\n",
    "\n",
    "    the continuous variable fnlwgt represents final weight, which is the number of units in the target population that the responding unit represents.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    dealing with missing values - do you drop those rows? Or do you try to impute the missing data? If so how? Explain why you chose the strategy you did.\n",
    "    Split your data into a test and training set.\n",
    "    Fit five different models to train and evaluate on test. For each model deciding how many features to keep and how many features to drop. Make sure you can explain why you dropped the features you did and why you kept the features you kept. Try running the model with and without the features you kept. Report the test precision, recall and f1 score.\n",
    "\n",
    "Each model should have a different number of layers, different activation functions, and different weight initializations.\n",
    "\n",
    "    Now try cross validation on the whole data set for each of the five models with and without the selected features. Does the feature selection lead to under or over fitting? How do you know?\n",
    "    Compare and contrast which features you kept and which ones you dropped, based on the model. Note: it may be the case that different features perform better with different models, so please explore keeping and dropping different features depending on the algorithm. How does your choice of model and features effect under or overfitting?\n",
    "    Try regularizing each of your models, does the generalizability increase? Decrease? In which cases does each happen and why? Please try this with all of your features and then with the reduced set of features. Report your precision, recall and f1 score on the train and test set. Next carry out cross validation. Does regularization reduce under or overfitting? Why or why not? How does the space of features your metrics and your optimal regularization parameters?\n",
    "\n",
    "Hint: you can use L1 or L2 norm for regularization or dropout.\n",
    "\n",
    "    Now instead of try different models we will use grid search and cross validation to tune the hyper parameters of our model. Our tunable parameters are:\n",
    "    The number of layers (please don't go deeper than 10 hidden layers)\n",
    "    The number of nodes per layer\n",
    "    The type of regularization to use\n",
    "    The type of weight initialization to use.\n",
    "    The type of activation function.\n",
    "    The metric to evaluate with, although logloss is standard, try using other metrics of accuracy. You may even try multiple and averaging or taking the harmonic weight of multiple metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
